# Apple Silicon ML Benchmarks

A comprehensive collection of benchmarks for machine learning models running on Apple Silicon machines (M2, M3 Ultra, M4 Max) using various tools and frameworks.

## 🎯 Project Goals

This project aims to:
- Collect and document performance benchmarks of ML models on Apple Silicon
- Compare different tools and frameworks (MLX, LLaMA LM Studio, LLaMA.cpp)
- Provide reproducible results for the community
- Help users make informed decisions about ML tools for Apple Silicon

## 🛠️ Tools & Frameworks

The project currently focuses on benchmarking using:
- [MLX](https://github.com/ml-explore/mlx) - Apple's machine learning framework
- [LLaMA LM Studio](https://lmstudio.ai/) - Local LLM inference and fine-tuning
- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp) - C++ implementation of LLaMA

## 📊 Benchmarks

The repository contains benchmarks for:
- Model inference speed
- Memory usage
- Token generation rates
- Model loading times
- Various model sizes and configurations

## 🤝 Contributing

Contributions are welcome! If you have benchmark results to share:
1. Fork the repository
2. Add your benchmark results with detailed specifications
3. Submit a pull request

Please include:
- Hardware specifications
- Software versions
- Model details
- Benchmark methodology
- Raw results

## 📝 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- Apple for their amazing Silicon chips
- The open-source community for developing these tools
- All contributors who share their benchmark results 